{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import os\n","import matplotlib.pyplot as plt\n","from tensorflow.keras.preprocessing import image\n","from keras.preprocessing.text import Tokenizer\n","import tensorflow as tf\n","from keras.preprocessing.sequence import pad_sequences"]},{"cell_type":"code","execution_count":2,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Empty DataFrame\n","Columns: [image_name,  comment_number,  comment]\n","Index: []\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>image_name</th>\n","      <th>comment_number</th>\n","      <th>comment</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1000092795.jpg</td>\n","      <td>0</td>\n","      <td>Two young guys with shaggy hair look at their...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1000092795.jpg</td>\n","      <td>1</td>\n","      <td>Two young , White males are outside near many...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1000092795.jpg</td>\n","      <td>2</td>\n","      <td>Two men in green shirts are standing in a yard .</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1000092795.jpg</td>\n","      <td>3</td>\n","      <td>A man in a blue shirt standing in a garden .</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1000092795.jpg</td>\n","      <td>4</td>\n","      <td>Two friends enjoy time spent together .</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["       image_name  comment_number  \\\n","0  1000092795.jpg               0   \n","1  1000092795.jpg               1   \n","2  1000092795.jpg               2   \n","3  1000092795.jpg               3   \n","4  1000092795.jpg               4   \n","\n","                                             comment  \n","0   Two young guys with shaggy hair look at their...  \n","1   Two young , White males are outside near many...  \n","2   Two men in green shirts are standing in a yard .  \n","3       A man in a blue shirt standing in a garden .  \n","4            Two friends enjoy time spent together .  "]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["metadata = pd.read_csv('flickr30k_dataset/results.csv',delimiter='|',engine='python')\n","metadata = metadata.dropna()\n","is_NaN = metadata.isnull()\n","row_has_NaN = is_NaN.any(axis=1)\n","rows_with_NaN = metadata[row_has_NaN]\n","print(rows_with_NaN)\n","metadata.head()"]},{"cell_type":"markdown","metadata":{},"source":["### Number of Samples"]},{"cell_type":"code","execution_count":3,"metadata":{"trusted":true},"outputs":[{"data":{"text/plain":["31783"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["len(metadata['image_name'].unique())"]},{"cell_type":"markdown","metadata":{},"source":["### Read the Data"]},{"cell_type":"code","execution_count":4,"metadata":{"trusted":true},"outputs":[],"source":["def load_image(name):\n","    img = image.load_img(name,target_size=(32,32,3))\n","    img = image.img_to_array(img)\n","    #img = img/255\n","    #plt.imshow(img)\n","    img = np.reshape(img,(32*32*3))\n","    return img"]},{"cell_type":"code","execution_count":5,"metadata":{"trusted":true},"outputs":[],"source":["image_arr = []\n","sentence_arr = []\n","for ind in range(5000):\n","    if ind % 5 != 0:\n","        continue\n","    image_location = (metadata.iloc[ind,:]['image_name'])\n","    sentence = (metadata.iloc[ind,:][' comment'])\n","    \n","    \n","    image_arr.append(load_image('flickr30k_dataset/flickr30k_images/flickr30k_images/'+str(image_location)) )\n","    sentence_arr.append('<SOS>'+sentence+'<EOS>')\n","    \n","        \n","Images =  np.array(image_arr)"]},{"cell_type":"markdown","metadata":{},"source":["# Preprocess\n","Converting to Word Embeddings\n","create padding and make equal length\n","Vocabulary\n","The complexity of the problem is determined by the complexity of the vocabulary. A more complex vocabulary is a more complex problem. Let's look at the complexity of the dataset we'll be working with.\n","\n","## Tokenize (IMPLEMENTATION)\n","For a neural network to predict on text data, it first has to be turned into data it can understand. Text data like \"dog\" is a sequence of ASCII character encodings. Since a neural network is a series of multiplication and addition operations, the input data needs to be number(s).\n","\n","We can turn each character into a number or each word into a number. These are called character and word ids, respectively. Character ids are used for character level models that generate text predictions for each character. A word level model uses word ids that generate text predictions for each word. Word level models tend to learn better, since they are lower in complexity, so we'll use those.\n","\n","Turn each sentence into a sequence of words ids using Keras's Tokenizer function. Use this function to tokenize english_sentences and french_sentences in the cell below.\n","\n","Running the cell will run tokenize on sample data and show output for debugging."]},{"cell_type":"code","execution_count":6,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["{'the': 1, 'quick': 2, 'a': 3, 'brown': 4, 'fox': 5, 'jumps': 6, 'over': 7, 'lazy': 8, 'dog': 9, 'by': 10, 'jove': 11, 'my': 12, 'study': 13, 'of': 14, 'lexicography': 15, 'won': 16, 'prize': 17, 'this': 18, 'is': 19, 'short': 20, 'sentence': 21}\n","\n","Sequence 1 in x\n","  Input:  The quick brown fox jumps over the lazy dog .\n","  Output: [1, 2, 4, 5, 6, 7, 1, 8, 9]\n","Sequence 2 in x\n","  Input:  By Jove , my quick study of lexicography won a prize .\n","  Output: [10, 11, 12, 2, 13, 14, 15, 16, 3, 17]\n","Sequence 3 in x\n","  Input:  This is a short sentence .\n","  Output: [18, 19, 3, 20, 21]\n"]}],"source":["def tokenize(x):\n","    \"\"\"\n","    Tokenize x\n","    :param x: List of sentences/strings to be tokenized\n","    :return: Tuple of (tokenized x data, tokenizer used to tokenize x)\n","    \"\"\"\n","    tokenizer=Tokenizer()\n","    tokenizer.fit_on_texts(x)\n","    t=tokenizer.texts_to_sequences(x)\n","    # TODO: Implement\n","    return t, tokenizer\n","\n","# Tokenize Example output\n","text_sentences = [\n","    'The quick brown fox jumps over the lazy dog .',\n","    'By Jove , my quick study of lexicography won a prize .',\n","    'This is a short sentence .']\n","text_tokenized, text_tokenizer = tokenize(text_sentences)\n","print(text_tokenizer.word_index)\n","print()\n","for sample_i, (sent, token_sent) in enumerate(zip(text_sentences, text_tokenized)):\n","    print('Sequence {} in x'.format(sample_i + 1))\n","    print('  Input:  {}'.format(sent))\n","    print('  Output: {}'.format(token_sent))"]},{"cell_type":"markdown","metadata":{},"source":["## Padding (IMPLEMENTATION)\n","When batching the sequence of word ids together, each sequence needs to be the same length. Since sentences are dynamic in length, we can add padding to the end of the sequences to make them the same length.\n","\n","Make sure all the English sequences have the same length and all the French sequences have the same length by adding padding to the end of each sequence using Keras's pad_sequences function."]},{"cell_type":"code","execution_count":7,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Sequence 1 in x\n","  Input:  [1 2 4 5 6 7 1 8 9]\n","  Output: [1 2 4 5 6 7 1 8 9 0]\n","Sequence 2 in x\n","  Input:  [10 11 12  2 13 14 15 16  3 17]\n","  Output: [10 11 12  2 13 14 15 16  3 17]\n","Sequence 3 in x\n","  Input:  [18 19  3 20 21]\n","  Output: [18 19  3 20 21  0  0  0  0  0]\n"]}],"source":["def pad(x, length=None):\n","    \"\"\"\n","    Pad x\n","    :param x: List of sequences.\n","    :param length: Length to pad the sequence to.  If None, use length of longest sequence in x.\n","    :return: Padded numpy array of sequences\n","    \"\"\"\n","    # TODO: Implement\n","    padding=pad_sequences(x,padding='post',maxlen=length)\n","    return padding\n","\n","# Pad Tokenized output\n","test_pad = pad(text_tokenized)\n","for sample_i, (token_sent, pad_sent) in enumerate(zip(text_tokenized, test_pad)):\n","    print('Sequence {} in x'.format(sample_i + 1))\n","    print('  Input:  {}'.format(np.array(token_sent)))\n","    print('  Output: {}'.format(pad_sent))"]},{"cell_type":"code","execution_count":8,"metadata":{"trusted":true},"outputs":[],"source":["def preprocess(sentences):\n","    text_tokenized, text_tokenizer = tokenize(sentences)\n","    text_pad = pad(text_tokenized)\n","    return text_pad, text_tokenizer\n","\n","Sentence , token_Sentence = preprocess(sentence_arr)"]},{"cell_type":"code","execution_count":9,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Sentence vocabulary size: 2302\n","Sentence Longest sentence size: 51\n"]}],"source":["print(\"Sentence vocabulary size:\", len(token_Sentence.word_index))\n","print(\"Sentence Longest sentence size:\", len(Sentence[0]))"]},{"cell_type":"code","execution_count":10,"metadata":{"trusted":true},"outputs":[{"data":{"text/plain":["((1000, 3072), (1000, 51))"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["Images.shape , Sentence.shape"]},{"cell_type":"code","execution_count":11,"metadata":{"trusted":true},"outputs":[{"data":{"text/plain":["array([[  2,  17,  21, ...,   0,   0,   0],\n","       [  2, 105,  37, ...,   0,   0,   0],\n","       [  2,   1,  49, ...,   0,   0,   0],\n","       ...,\n","       [  2,   1, 296, ...,   0,   0,   0],\n","       [  2,   1,  21, ...,   0,   0,   0],\n","       [  2,   1,  15, ...,   0,   0,   0]])"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["Sentence"]},{"cell_type":"markdown","metadata":{},"source":["### Load and batch data\n","This tutorial uses torchtext to generate Wikitext-2 dataset. The vocab object is built based on the train dataset and is used to numericalize tokens into tensors. Starting from sequential data, the batchify() function arranges the dataset into columns, trimming off any tokens remaining after the data has been divided into batches of size batch_size. For instance, with the alphabet as the sequence (total length of 26) and a batch size of 4, we would divide the alphabet into 4 sequences of length 6:\n","\n","\n"," \n"," \n"," \n","These columns are treated as independent by the model, which means that the dependence of G and F can not be learned, but allows more efficient batch processing."]},{"cell_type":"code","execution_count":12,"metadata":{"trusted":true},"outputs":[],"source":["def create_batch(src, tar , batchsize , i):\n","    src, tar =  np.transpose(src[(i-1)*batchsize : (i-1)*batchsize + batchsize]) , np.transpose(tar[(i-1)*batchsize : (i-1)*batchsize + batchsize])\n","    return torch.tensor(src).long(),torch.tensor(tar).long()"]},{"cell_type":"markdown","metadata":{},"source":["# Modeling Transformer"]},{"cell_type":"code","execution_count":13,"metadata":{"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\bhatt\\anaconda3\\envs\\dLenv\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","class Transformer(nn.Module):\n","    def __init__(\n","        self,\n","        embedding_size,\n","        src_vocab_size,\n","        trg_vocab_size,\n","        src_pad_idx,\n","        num_heads,\n","        num_encoder_layers,\n","        num_decoder_layers,\n","        forward_expansion,\n","        dropout,\n","        max_len_s,\n","        max_len_t,\n","        device,\n","    ):\n","        super(Transformer, self).__init__()\n","        self.src_word_embedding = nn.Embedding(src_vocab_size, embedding_size)\n","        self.src_position_embedding = nn.Embedding(max_len_s, embedding_size)\n","        self.trg_word_embedding = nn.Embedding(trg_vocab_size, embedding_size)\n","        self.trg_position_embedding = nn.Embedding(max_len_t, embedding_size)\n","\n","        self.device = device\n","        self.transformer = nn.Transformer(\n","            embedding_size,\n","            num_heads,\n","            num_encoder_layers,\n","            num_decoder_layers,\n","            forward_expansion,\n","            dropout,\n","        )\n","        self.fc_out = nn.Linear(embedding_size, trg_vocab_size)\n","        self.dropout = nn.Dropout(dropout)\n","        self.src_pad_idx = src_pad_idx\n","\n","    def make_src_mask(self, src):\n","        src_mask = src.transpose(0, 1) == self.src_pad_idx\n","\n","        # (N, src_len)\n","        return src_mask.to(self.device)\n","\n","    def forward(self, src, trg):\n","        src_seq_length, N = src.shape\n","        trg_seq_length, N = trg.shape\n","\n","        src_positions = (\n","            torch.arange(0, src_seq_length)\n","            .unsqueeze(1)\n","            .expand(src_seq_length, N)\n","            .to(self.device)\n","        )\n","\n","        trg_positions = (\n","            torch.arange(0, trg_seq_length)\n","            .unsqueeze(1)\n","            .expand(trg_seq_length, N)\n","            .to(self.device)\n","        )\n","\n","        embed_src = self.dropout(\n","            (self.src_word_embedding(src) + self.src_position_embedding(src_positions))\n","        )\n","        embed_trg = self.dropout(\n","            (self.trg_word_embedding(trg) + self.trg_position_embedding(trg_positions))\n","        )\n","\n","        src_padding_mask = self.make_src_mask(src)\n","        trg_mask = self.transformer.generate_square_subsequent_mask(trg_seq_length).to(self.device)\n","        out = self.transformer(\n","            embed_src,\n","            embed_trg,\n","            src_key_padding_mask=src_padding_mask,\n","            tgt_mask=trg_mask,\n","        )\n","        out = self.fc_out(out)\n","        return out"]},{"cell_type":"code","execution_count":14,"metadata":{"trusted":true},"outputs":[{"data":{"text/plain":["device(type='cuda')"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","device"]},{"cell_type":"code","execution_count":15,"metadata":{"trusted":true},"outputs":[],"source":["# Model hyperparameters\n","src_vocab_size = 256\n","trg_vocab_size = len(token_Sentence.word_index)\n","embedding_size = 512\n","num_heads = 8\n","num_encoder_layers = 3\n","num_decoder_layers = 3\n","dropout = 0.10\n","max_len_s = Images.shape[1]\n","max_len_t = len(Sentence[0])\n","forward_expansion = 4\n","src_pad_idx = 0\n"]},{"cell_type":"code","execution_count":17,"metadata":{"trusted":true},"outputs":[],"source":["# Training hyperparameters\n","num_epochs = 10000\n","learning_rate = 3e-4\n","batch_size = 5"]},{"cell_type":"code","execution_count":18,"metadata":{"trusted":true},"outputs":[],"source":["model = Transformer(\n","    embedding_size,\n","    src_vocab_size,\n","    trg_vocab_size,\n","    src_pad_idx,\n","    num_heads,\n","    num_encoder_layers,\n","    num_decoder_layers,\n","    forward_expansion,\n","    dropout,\n","    max_len_s,\n","    max_len_t,\n","    device,\n",").to(device)\n","\n","optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","\n","scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n","    optimizer, factor=0.1, patience=10, verbose=True\n",")\n","\n","pad_idx = 0\n","criterion = nn.CrossEntropyLoss(ignore_index=pad_idx).cuda()\n"]},{"cell_type":"code","execution_count":19,"metadata":{"trusted":true},"outputs":[],"source":["import time\n","def train():\n","    model.train() # Turn on the train mode\n","    total_loss = 0\n","    start_time = time.time()\n","    for i in range(1, 999):\n","        src,tar = create_batch(Images,Sentence, batch_size , i)\n","        src = src.to(device)\n","        tar = tar.to(device)\n","        optimizer.zero_grad()\n","        output = model(src,tar)\n","        loss = criterion(output.view(-1, output.shape[2]), tar.reshape(-1))\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n","        optimizer.step()\n","        cur_loss = loss.item()\n","        total_loss += cur_loss\n","        log_interval = 100\n","        if i % log_interval == 0 and i > 0:\n","            elapsed = time.time() - start_time\n","            print('| epoch {:3d} | {:5d}/{:5d} batches | '\n","                  's/batch {:5.2f} | '\n","                  'loss {:5.2f} | ppl {:8.2f}'.format(\n","                    epoch, i, (src.shape[1]) // batch_size, \n","                    elapsed  / log_interval,\n","                    cur_loss, math.exp(cur_loss)))\n","            start_time = time.time()\n","    return total_loss"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[],"source":["torch.cuda.empty_cache()\n"]},{"cell_type":"code","execution_count":22,"metadata":{"trusted":true},"outputs":[{"ename":"RuntimeError","evalue":"CUDA out of memory. Tried to allocate 1.41 GiB (GPU 0; 4.00 GiB total capacity; 3.17 GiB already allocated; 0 bytes free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[1;32me:\\Final_YearProject\\ImageCaptionGenerator\\transformer-based-image-captioning.ipynb Cell 26'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Final_YearProject/ImageCaptionGenerator/transformer-based-image-captioning.ipynb#ch0000025?line=1'>2</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39m30\u001b[39m):\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Final_YearProject/ImageCaptionGenerator/transformer-based-image-captioning.ipynb#ch0000025?line=2'>3</a>\u001b[0m     epoch_start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/Final_YearProject/ImageCaptionGenerator/transformer-based-image-captioning.ipynb#ch0000025?line=3'>4</a>\u001b[0m     loss \u001b[39m=\u001b[39m train()\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Final_YearProject/ImageCaptionGenerator/transformer-based-image-captioning.ipynb#ch0000025?line=4'>5</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m-\u001b[39m\u001b[39m'\u001b[39m \u001b[39m*\u001b[39m \u001b[39m89\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Final_YearProject/ImageCaptionGenerator/transformer-based-image-captioning.ipynb#ch0000025?line=5'>6</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m| end of epoch \u001b[39m\u001b[39m{:3d}\u001b[39;00m\u001b[39m | time: \u001b[39m\u001b[39m{:5.2f}\u001b[39;00m\u001b[39ms | Training loss \u001b[39m\u001b[39m{:5.2f}\u001b[39;00m\u001b[39m | \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Final_YearProject/ImageCaptionGenerator/transformer-based-image-captioning.ipynb#ch0000025?line=6'>7</a>\u001b[0m           \u001b[39m.\u001b[39mformat(epoch, (time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m epoch_start_time),\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Final_YearProject/ImageCaptionGenerator/transformer-based-image-captioning.ipynb#ch0000025?line=7'>8</a>\u001b[0m                                      loss))\n","\u001b[1;32me:\\Final_YearProject\\ImageCaptionGenerator\\transformer-based-image-captioning.ipynb Cell 24'\u001b[0m in \u001b[0;36mtrain\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Final_YearProject/ImageCaptionGenerator/transformer-based-image-captioning.ipynb#ch0000024?line=8'>9</a>\u001b[0m tar \u001b[39m=\u001b[39m tar\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Final_YearProject/ImageCaptionGenerator/transformer-based-image-captioning.ipynb#ch0000024?line=9'>10</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/Final_YearProject/ImageCaptionGenerator/transformer-based-image-captioning.ipynb#ch0000024?line=10'>11</a>\u001b[0m output \u001b[39m=\u001b[39m model(src,tar)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Final_YearProject/ImageCaptionGenerator/transformer-based-image-captioning.ipynb#ch0000024?line=11'>12</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(output\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, output\u001b[39m.\u001b[39mshape[\u001b[39m2\u001b[39m]), tar\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m))\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Final_YearProject/ImageCaptionGenerator/transformer-based-image-captioning.ipynb#ch0000024?line=12'>13</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n","File \u001b[1;32mc:\\Users\\bhatt\\anaconda3\\envs\\dLenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","\u001b[1;32me:\\Final_YearProject\\ImageCaptionGenerator\\transformer-based-image-captioning.ipynb Cell 19'\u001b[0m in \u001b[0;36mTransformer.forward\u001b[1;34m(self, src, trg)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Final_YearProject/ImageCaptionGenerator/transformer-based-image-captioning.ipynb#ch0000019?line=69'>70</a>\u001b[0m src_padding_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmake_src_mask(src)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Final_YearProject/ImageCaptionGenerator/transformer-based-image-captioning.ipynb#ch0000019?line=70'>71</a>\u001b[0m trg_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransformer\u001b[39m.\u001b[39mgenerate_square_subsequent_mask(trg_seq_length)\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/Final_YearProject/ImageCaptionGenerator/transformer-based-image-captioning.ipynb#ch0000019?line=71'>72</a>\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer(\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Final_YearProject/ImageCaptionGenerator/transformer-based-image-captioning.ipynb#ch0000019?line=72'>73</a>\u001b[0m     embed_src,\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Final_YearProject/ImageCaptionGenerator/transformer-based-image-captioning.ipynb#ch0000019?line=73'>74</a>\u001b[0m     embed_trg,\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Final_YearProject/ImageCaptionGenerator/transformer-based-image-captioning.ipynb#ch0000019?line=74'>75</a>\u001b[0m     src_key_padding_mask\u001b[39m=\u001b[39;49msrc_padding_mask,\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Final_YearProject/ImageCaptionGenerator/transformer-based-image-captioning.ipynb#ch0000019?line=75'>76</a>\u001b[0m     tgt_mask\u001b[39m=\u001b[39;49mtrg_mask,\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Final_YearProject/ImageCaptionGenerator/transformer-based-image-captioning.ipynb#ch0000019?line=76'>77</a>\u001b[0m )\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Final_YearProject/ImageCaptionGenerator/transformer-based-image-captioning.ipynb#ch0000019?line=77'>78</a>\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc_out(out)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Final_YearProject/ImageCaptionGenerator/transformer-based-image-captioning.ipynb#ch0000019?line=78'>79</a>\u001b[0m \u001b[39mreturn\u001b[39;00m out\n","File \u001b[1;32mc:\\Users\\bhatt\\anaconda3\\envs\\dLenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[1;32mc:\\Users\\bhatt\\anaconda3\\envs\\dLenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:145\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[1;34m(self, src, tgt, src_mask, tgt_mask, memory_mask, src_key_padding_mask, tgt_key_padding_mask, memory_key_padding_mask)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/transformer.py?line=141'>142</a>\u001b[0m \u001b[39mif\u001b[39;00m src\u001b[39m.\u001b[39msize(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39md_model \u001b[39mor\u001b[39;00m tgt\u001b[39m.\u001b[39msize(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39md_model:\n\u001b[0;32m    <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/transformer.py?line=142'>143</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mthe feature number of src and tgt must be equal to d_model\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/transformer.py?line=144'>145</a>\u001b[0m memory \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(src, mask\u001b[39m=\u001b[39;49msrc_mask, src_key_padding_mask\u001b[39m=\u001b[39;49msrc_key_padding_mask)\n\u001b[0;32m    <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/transformer.py?line=145'>146</a>\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder(tgt, memory, tgt_mask\u001b[39m=\u001b[39mtgt_mask, memory_mask\u001b[39m=\u001b[39mmemory_mask,\n\u001b[0;32m    <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/transformer.py?line=146'>147</a>\u001b[0m                       tgt_key_padding_mask\u001b[39m=\u001b[39mtgt_key_padding_mask,\n\u001b[0;32m    <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/transformer.py?line=147'>148</a>\u001b[0m                       memory_key_padding_mask\u001b[39m=\u001b[39mmemory_key_padding_mask)\n\u001b[0;32m    <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/transformer.py?line=148'>149</a>\u001b[0m \u001b[39mreturn\u001b[39;00m output\n","File \u001b[1;32mc:\\Users\\bhatt\\anaconda3\\envs\\dLenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[1;32mc:\\Users\\bhatt\\anaconda3\\envs\\dLenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:202\u001b[0m, in \u001b[0;36mTransformerEncoder.forward\u001b[1;34m(self, src, mask, src_key_padding_mask)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/transformer.py?line=198'>199</a>\u001b[0m output \u001b[39m=\u001b[39m src\n\u001b[0;32m    <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/transformer.py?line=200'>201</a>\u001b[0m \u001b[39mfor\u001b[39;00m mod \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers:\n\u001b[1;32m--> <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/transformer.py?line=201'>202</a>\u001b[0m     output \u001b[39m=\u001b[39m mod(output, src_mask\u001b[39m=\u001b[39;49mmask, src_key_padding_mask\u001b[39m=\u001b[39;49msrc_key_padding_mask)\n\u001b[0;32m    <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/transformer.py?line=203'>204</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/transformer.py?line=204'>205</a>\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm(output)\n","File \u001b[1;32mc:\\Users\\bhatt\\anaconda3\\envs\\dLenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[1;32mc:\\Users\\bhatt\\anaconda3\\envs\\dLenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:344\u001b[0m, in \u001b[0;36mTransformerEncoderLayer.forward\u001b[1;34m(self, src, src_mask, src_key_padding_mask)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/transformer.py?line=341'>342</a>\u001b[0m     x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ff_block(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm2(x))\n\u001b[0;32m    <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/transformer.py?line=342'>343</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/transformer.py?line=343'>344</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm1(x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sa_block(x, src_mask, src_key_padding_mask))\n\u001b[0;32m    <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/transformer.py?line=344'>345</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm2(x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ff_block(x))\n\u001b[0;32m    <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/transformer.py?line=346'>347</a>\u001b[0m \u001b[39mreturn\u001b[39;00m x\n","File \u001b[1;32mc:\\Users\\bhatt\\anaconda3\\envs\\dLenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:352\u001b[0m, in \u001b[0;36mTransformerEncoderLayer._sa_block\u001b[1;34m(self, x, attn_mask, key_padding_mask)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/transformer.py?line=349'>350</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_sa_block\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor,\n\u001b[0;32m    <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/transformer.py?line=350'>351</a>\u001b[0m               attn_mask: Optional[Tensor], key_padding_mask: Optional[Tensor]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/transformer.py?line=351'>352</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself_attn(x, x, x,\n\u001b[0;32m    <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/transformer.py?line=352'>353</a>\u001b[0m                        attn_mask\u001b[39m=\u001b[39;49mattn_mask,\n\u001b[0;32m    <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/transformer.py?line=353'>354</a>\u001b[0m                        key_padding_mask\u001b[39m=\u001b[39;49mkey_padding_mask,\n\u001b[0;32m    <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/transformer.py?line=354'>355</a>\u001b[0m                        need_weights\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)[\u001b[39m0\u001b[39m]\n\u001b[0;32m    <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/transformer.py?line=355'>356</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout1(x)\n","File \u001b[1;32mc:\\Users\\bhatt\\anaconda3\\envs\\dLenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[1;32mc:\\Users\\bhatt\\anaconda3\\envs\\dLenv\\lib\\site-packages\\torch\\nn\\modules\\activation.py:1038\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[1;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/activation.py?line=1026'>1027</a>\u001b[0m     attn_output, attn_output_weights \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mmulti_head_attention_forward(\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/activation.py?line=1027'>1028</a>\u001b[0m         query, key, value, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_dim, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads,\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/activation.py?line=1028'>1029</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39min_proj_weight, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39min_proj_bias,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/activation.py?line=1034'>1035</a>\u001b[0m         q_proj_weight\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mq_proj_weight, k_proj_weight\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mk_proj_weight,\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/activation.py?line=1035'>1036</a>\u001b[0m         v_proj_weight\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mv_proj_weight, average_attn_weights\u001b[39m=\u001b[39maverage_attn_weights)\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/activation.py?line=1036'>1037</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/activation.py?line=1037'>1038</a>\u001b[0m     attn_output, attn_output_weights \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39;49mmulti_head_attention_forward(\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/activation.py?line=1038'>1039</a>\u001b[0m         query, key, value, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membed_dim, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_heads,\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/activation.py?line=1039'>1040</a>\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49min_proj_weight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49min_proj_bias,\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/activation.py?line=1040'>1041</a>\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias_k, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias_v, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madd_zero_attn,\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/activation.py?line=1041'>1042</a>\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mout_proj\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mout_proj\u001b[39m.\u001b[39;49mbias,\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/activation.py?line=1042'>1043</a>\u001b[0m         training\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining,\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/activation.py?line=1043'>1044</a>\u001b[0m         key_padding_mask\u001b[39m=\u001b[39;49mkey_padding_mask, need_weights\u001b[39m=\u001b[39;49mneed_weights,\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/activation.py?line=1044'>1045</a>\u001b[0m         attn_mask\u001b[39m=\u001b[39;49mattn_mask, average_attn_weights\u001b[39m=\u001b[39;49maverage_attn_weights)\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/activation.py?line=1045'>1046</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_first \u001b[39mand\u001b[39;00m is_batched:\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/activation.py?line=1046'>1047</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m attn_output\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m), attn_output_weights\n","File \u001b[1;32mc:\\Users\\bhatt\\anaconda3\\envs\\dLenv\\lib\\site-packages\\torch\\nn\\functional.py:5358\u001b[0m, in \u001b[0;36mmulti_head_attention_forward\u001b[1;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/functional.py?line=5352'>5353</a>\u001b[0m     dropout_p \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/functional.py?line=5354'>5355</a>\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/functional.py?line=5355'>5356</a>\u001b[0m \u001b[39m# (deep breath) calculate attention and out projection\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/functional.py?line=5356'>5357</a>\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[1;32m-> <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/functional.py?line=5357'>5358</a>\u001b[0m attn_output, attn_output_weights \u001b[39m=\u001b[39m _scaled_dot_product_attention(q, k, v, attn_mask, dropout_p)\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/functional.py?line=5358'>5359</a>\u001b[0m attn_output \u001b[39m=\u001b[39m attn_output\u001b[39m.\u001b[39mtranspose(\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mcontiguous()\u001b[39m.\u001b[39mview(tgt_len, bsz, embed_dim)\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/functional.py?line=5359'>5360</a>\u001b[0m attn_output \u001b[39m=\u001b[39m linear(attn_output, out_proj_weight, out_proj_bias)\n","File \u001b[1;32mc:\\Users\\bhatt\\anaconda3\\envs\\dLenv\\lib\\site-packages\\torch\\nn\\functional.py:5037\u001b[0m, in \u001b[0;36m_scaled_dot_product_attention\u001b[1;34m(q, k, v, attn_mask, dropout_p)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/functional.py?line=5034'>5035</a>\u001b[0m \u001b[39mif\u001b[39;00m attn_mask \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/functional.py?line=5035'>5036</a>\u001b[0m     attn \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m attn_mask\n\u001b[1;32m-> <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/functional.py?line=5036'>5037</a>\u001b[0m attn \u001b[39m=\u001b[39m softmax(attn, dim\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/functional.py?line=5037'>5038</a>\u001b[0m \u001b[39mif\u001b[39;00m dropout_p \u001b[39m>\u001b[39m \u001b[39m0.0\u001b[39m:\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/functional.py?line=5038'>5039</a>\u001b[0m     attn \u001b[39m=\u001b[39m dropout(attn, p\u001b[39m=\u001b[39mdropout_p)\n","File \u001b[1;32mc:\\Users\\bhatt\\anaconda3\\envs\\dLenv\\lib\\site-packages\\torch\\nn\\functional.py:1818\u001b[0m, in \u001b[0;36msoftmax\u001b[1;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/functional.py?line=1815'>1816</a>\u001b[0m     dim \u001b[39m=\u001b[39m _get_softmax_dim(\u001b[39m\"\u001b[39m\u001b[39msoftmax\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mdim(), _stacklevel)\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/functional.py?line=1816'>1817</a>\u001b[0m \u001b[39mif\u001b[39;00m dtype \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/functional.py?line=1817'>1818</a>\u001b[0m     ret \u001b[39m=\u001b[39m \u001b[39minput\u001b[39;49m\u001b[39m.\u001b[39;49msoftmax(dim)\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/functional.py?line=1818'>1819</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/functional.py?line=1819'>1820</a>\u001b[0m     ret \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msoftmax(dim, dtype\u001b[39m=\u001b[39mdtype)\n","\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 1.41 GiB (GPU 0; 4.00 GiB total capacity; 3.17 GiB already allocated; 0 bytes free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"]}],"source":["import math\n","for epoch in range(1, 30):\n","    epoch_start_time = time.time()\n","    loss = train()\n","    print('-' * 89)\n","    print('| end of epoch {:3d} | time: {:5.2f}s | Training loss {:5.2f} | '\n","          .format(epoch, (time.time() - epoch_start_time),\n","                                     loss))"]},{"cell_type":"markdown","metadata":{},"source":["# Evaluate\n","The following steps are used for evaluation:\n","\n","1. Encode english sentence\n","2. Add tokens of start and end\n","3. Decoder input is start token SOS\n","4. Get the padded version of enoded sentences\n","5. create mask\n","Till get eos token calculate or create sentence"]},{"cell_type":"code","execution_count":22,"metadata":{"trusted":true},"outputs":[],"source":["def display_image(name):\n","    img = image.load_img(name,target_size=(512,512,3))\n","    img = image.img_to_array(img)\n","    img = img/255\n","    plt.imshow(img)"]},{"cell_type":"code","execution_count":24,"metadata":{"trusted":true},"outputs":[],"source":["def evaluate(index):\n","    image_location, sent = metadata.iloc[index,0],metadata.iloc[index,2]\n","    image_arr = []\n","    img = load_image('flickr30k_dataset/flickr30k_images/flickr30k_images/'+str(image_location))\n","    image_arr.append(img)\n","    img_arr = np.array(image_arr)\n","    sentence = []\n","    sentence.append(sent)\n","    sentence[0] = '<SOS> '+sentence[0]+'<EOS>'\n","    sentence = pad(token_Sentence.texts_to_sequences(sentence) , length = max_len_t)\n","    src , tar = create_batch(img_arr,sentence, 1,1)\n","    src = src.to(device)\n","    tar = tar.to(device)\n","    model.eval()\n","    output =  model(src,tar)\n","    loss = criterion(output.view(-1, output.shape[2]), tar.reshape(-1))\n","    sentence_formed = ''\n","    val, ind = torch.max(output.view(-1, output.shape[2]), 1)\n","    for word in ind:\n","        #print('--->'+sentence_formed+'    '+str(word.item()))\n","        if word.item() == 3: # EOS\n","                break\n","        for key, value in token_Sentence.word_index.items():\n","            #print(value == word.item())\n","            if value == word.item() and value != 2: # sos\n","                sentence_formed = sentence_formed + key +' '\n","                break\n","    display_image('flickr30k_dataset/flickr30k_images/flickr30k_images/'+str(image_location))\n","    return sentence_formed , loss"]},{"cell_type":"code","execution_count":27,"metadata":{"trusted":true},"outputs":[{"ename":"RuntimeError","evalue":"CUDA out of memory. Tried to allocate 288.00 MiB (GPU 0; 4.00 GiB total capacity; 2.94 GiB already allocated; 0 bytes free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[1;32me:\\Final_YearProject\\ImageCaptionGenerator\\transformer-based-image-captioning.ipynb Cell 30'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/Final_YearProject/ImageCaptionGenerator/transformer-based-image-captioning.ipynb#ch0000029?line=0'>1</a>\u001b[0m evaluate(\u001b[39m76\u001b[39;49m)\n","\u001b[1;32me:\\Final_YearProject\\ImageCaptionGenerator\\transformer-based-image-captioning.ipynb Cell 29'\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(index)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Final_YearProject/ImageCaptionGenerator/transformer-based-image-captioning.ipynb#ch0000028?line=12'>13</a>\u001b[0m tar \u001b[39m=\u001b[39m tar\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Final_YearProject/ImageCaptionGenerator/transformer-based-image-captioning.ipynb#ch0000028?line=13'>14</a>\u001b[0m model\u001b[39m.\u001b[39meval()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/Final_YearProject/ImageCaptionGenerator/transformer-based-image-captioning.ipynb#ch0000028?line=14'>15</a>\u001b[0m output \u001b[39m=\u001b[39m  model(src,tar)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Final_YearProject/ImageCaptionGenerator/transformer-based-image-captioning.ipynb#ch0000028?line=15'>16</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(output\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, output\u001b[39m.\u001b[39mshape[\u001b[39m2\u001b[39m]), tar\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m))\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Final_YearProject/ImageCaptionGenerator/transformer-based-image-captioning.ipynb#ch0000028?line=16'>17</a>\u001b[0m sentence_formed \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m\n","File \u001b[1;32mc:\\Users\\bhatt\\anaconda3\\envs\\dLenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","\u001b[1;32me:\\Final_YearProject\\ImageCaptionGenerator\\transformer-based-image-captioning.ipynb Cell 20'\u001b[0m in \u001b[0;36mTransformer.forward\u001b[1;34m(self, src, trg)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Final_YearProject/ImageCaptionGenerator/transformer-based-image-captioning.ipynb#ch0000019?line=69'>70</a>\u001b[0m src_padding_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmake_src_mask(src)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Final_YearProject/ImageCaptionGenerator/transformer-based-image-captioning.ipynb#ch0000019?line=70'>71</a>\u001b[0m trg_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransformer\u001b[39m.\u001b[39mgenerate_square_subsequent_mask(trg_seq_length)\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/Final_YearProject/ImageCaptionGenerator/transformer-based-image-captioning.ipynb#ch0000019?line=71'>72</a>\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer(\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Final_YearProject/ImageCaptionGenerator/transformer-based-image-captioning.ipynb#ch0000019?line=72'>73</a>\u001b[0m     embed_src,\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Final_YearProject/ImageCaptionGenerator/transformer-based-image-captioning.ipynb#ch0000019?line=73'>74</a>\u001b[0m     embed_trg,\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Final_YearProject/ImageCaptionGenerator/transformer-based-image-captioning.ipynb#ch0000019?line=74'>75</a>\u001b[0m     src_key_padding_mask\u001b[39m=\u001b[39;49msrc_padding_mask,\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Final_YearProject/ImageCaptionGenerator/transformer-based-image-captioning.ipynb#ch0000019?line=75'>76</a>\u001b[0m     tgt_mask\u001b[39m=\u001b[39;49mtrg_mask,\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Final_YearProject/ImageCaptionGenerator/transformer-based-image-captioning.ipynb#ch0000019?line=76'>77</a>\u001b[0m )\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Final_YearProject/ImageCaptionGenerator/transformer-based-image-captioning.ipynb#ch0000019?line=77'>78</a>\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc_out(out)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Final_YearProject/ImageCaptionGenerator/transformer-based-image-captioning.ipynb#ch0000019?line=78'>79</a>\u001b[0m \u001b[39mreturn\u001b[39;00m out\n","File \u001b[1;32mc:\\Users\\bhatt\\anaconda3\\envs\\dLenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[1;32mc:\\Users\\bhatt\\anaconda3\\envs\\dLenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:145\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[1;34m(self, src, tgt, src_mask, tgt_mask, memory_mask, src_key_padding_mask, tgt_key_padding_mask, memory_key_padding_mask)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/transformer.py?line=141'>142</a>\u001b[0m \u001b[39mif\u001b[39;00m src\u001b[39m.\u001b[39msize(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39md_model \u001b[39mor\u001b[39;00m tgt\u001b[39m.\u001b[39msize(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39md_model:\n\u001b[0;32m    <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/transformer.py?line=142'>143</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mthe feature number of src and tgt must be equal to d_model\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/transformer.py?line=144'>145</a>\u001b[0m memory \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(src, mask\u001b[39m=\u001b[39;49msrc_mask, src_key_padding_mask\u001b[39m=\u001b[39;49msrc_key_padding_mask)\n\u001b[0;32m    <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/transformer.py?line=145'>146</a>\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder(tgt, memory, tgt_mask\u001b[39m=\u001b[39mtgt_mask, memory_mask\u001b[39m=\u001b[39mmemory_mask,\n\u001b[0;32m    <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/transformer.py?line=146'>147</a>\u001b[0m                       tgt_key_padding_mask\u001b[39m=\u001b[39mtgt_key_padding_mask,\n\u001b[0;32m    <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/transformer.py?line=147'>148</a>\u001b[0m                       memory_key_padding_mask\u001b[39m=\u001b[39mmemory_key_padding_mask)\n\u001b[0;32m    <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/transformer.py?line=148'>149</a>\u001b[0m \u001b[39mreturn\u001b[39;00m output\n","File \u001b[1;32mc:\\Users\\bhatt\\anaconda3\\envs\\dLenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[1;32mc:\\Users\\bhatt\\anaconda3\\envs\\dLenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:202\u001b[0m, in \u001b[0;36mTransformerEncoder.forward\u001b[1;34m(self, src, mask, src_key_padding_mask)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/transformer.py?line=198'>199</a>\u001b[0m output \u001b[39m=\u001b[39m src\n\u001b[0;32m    <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/transformer.py?line=200'>201</a>\u001b[0m \u001b[39mfor\u001b[39;00m mod \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers:\n\u001b[1;32m--> <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/transformer.py?line=201'>202</a>\u001b[0m     output \u001b[39m=\u001b[39m mod(output, src_mask\u001b[39m=\u001b[39;49mmask, src_key_padding_mask\u001b[39m=\u001b[39;49msrc_key_padding_mask)\n\u001b[0;32m    <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/transformer.py?line=203'>204</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/transformer.py?line=204'>205</a>\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm(output)\n","File \u001b[1;32mc:\\Users\\bhatt\\anaconda3\\envs\\dLenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[1;32mc:\\Users\\bhatt\\anaconda3\\envs\\dLenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:344\u001b[0m, in \u001b[0;36mTransformerEncoderLayer.forward\u001b[1;34m(self, src, src_mask, src_key_padding_mask)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/transformer.py?line=341'>342</a>\u001b[0m     x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ff_block(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm2(x))\n\u001b[0;32m    <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/transformer.py?line=342'>343</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/transformer.py?line=343'>344</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm1(x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sa_block(x, src_mask, src_key_padding_mask))\n\u001b[0;32m    <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/transformer.py?line=344'>345</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm2(x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ff_block(x))\n\u001b[0;32m    <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/transformer.py?line=346'>347</a>\u001b[0m \u001b[39mreturn\u001b[39;00m x\n","File \u001b[1;32mc:\\Users\\bhatt\\anaconda3\\envs\\dLenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:352\u001b[0m, in \u001b[0;36mTransformerEncoderLayer._sa_block\u001b[1;34m(self, x, attn_mask, key_padding_mask)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/transformer.py?line=349'>350</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_sa_block\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor,\n\u001b[0;32m    <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/transformer.py?line=350'>351</a>\u001b[0m               attn_mask: Optional[Tensor], key_padding_mask: Optional[Tensor]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/transformer.py?line=351'>352</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself_attn(x, x, x,\n\u001b[0;32m    <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/transformer.py?line=352'>353</a>\u001b[0m                        attn_mask\u001b[39m=\u001b[39;49mattn_mask,\n\u001b[0;32m    <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/transformer.py?line=353'>354</a>\u001b[0m                        key_padding_mask\u001b[39m=\u001b[39;49mkey_padding_mask,\n\u001b[0;32m    <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/transformer.py?line=354'>355</a>\u001b[0m                        need_weights\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)[\u001b[39m0\u001b[39m]\n\u001b[0;32m    <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/transformer.py?line=355'>356</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout1(x)\n","File \u001b[1;32mc:\\Users\\bhatt\\anaconda3\\envs\\dLenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[1;32mc:\\Users\\bhatt\\anaconda3\\envs\\dLenv\\lib\\site-packages\\torch\\nn\\modules\\activation.py:1038\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[1;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/activation.py?line=1026'>1027</a>\u001b[0m     attn_output, attn_output_weights \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mmulti_head_attention_forward(\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/activation.py?line=1027'>1028</a>\u001b[0m         query, key, value, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_dim, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads,\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/activation.py?line=1028'>1029</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39min_proj_weight, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39min_proj_bias,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/activation.py?line=1034'>1035</a>\u001b[0m         q_proj_weight\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mq_proj_weight, k_proj_weight\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mk_proj_weight,\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/activation.py?line=1035'>1036</a>\u001b[0m         v_proj_weight\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mv_proj_weight, average_attn_weights\u001b[39m=\u001b[39maverage_attn_weights)\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/activation.py?line=1036'>1037</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/activation.py?line=1037'>1038</a>\u001b[0m     attn_output, attn_output_weights \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39;49mmulti_head_attention_forward(\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/activation.py?line=1038'>1039</a>\u001b[0m         query, key, value, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membed_dim, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_heads,\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/activation.py?line=1039'>1040</a>\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49min_proj_weight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49min_proj_bias,\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/activation.py?line=1040'>1041</a>\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias_k, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias_v, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madd_zero_attn,\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/activation.py?line=1041'>1042</a>\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mout_proj\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mout_proj\u001b[39m.\u001b[39;49mbias,\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/activation.py?line=1042'>1043</a>\u001b[0m         training\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining,\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/activation.py?line=1043'>1044</a>\u001b[0m         key_padding_mask\u001b[39m=\u001b[39;49mkey_padding_mask, need_weights\u001b[39m=\u001b[39;49mneed_weights,\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/activation.py?line=1044'>1045</a>\u001b[0m         attn_mask\u001b[39m=\u001b[39;49mattn_mask, average_attn_weights\u001b[39m=\u001b[39;49maverage_attn_weights)\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/activation.py?line=1045'>1046</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_first \u001b[39mand\u001b[39;00m is_batched:\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/activation.py?line=1046'>1047</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m attn_output\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m), attn_output_weights\n","File \u001b[1;32mc:\\Users\\bhatt\\anaconda3\\envs\\dLenv\\lib\\site-packages\\torch\\nn\\functional.py:5358\u001b[0m, in \u001b[0;36mmulti_head_attention_forward\u001b[1;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/functional.py?line=5352'>5353</a>\u001b[0m     dropout_p \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/functional.py?line=5354'>5355</a>\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/functional.py?line=5355'>5356</a>\u001b[0m \u001b[39m# (deep breath) calculate attention and out projection\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/functional.py?line=5356'>5357</a>\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[1;32m-> <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/functional.py?line=5357'>5358</a>\u001b[0m attn_output, attn_output_weights \u001b[39m=\u001b[39m _scaled_dot_product_attention(q, k, v, attn_mask, dropout_p)\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/functional.py?line=5358'>5359</a>\u001b[0m attn_output \u001b[39m=\u001b[39m attn_output\u001b[39m.\u001b[39mtranspose(\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mcontiguous()\u001b[39m.\u001b[39mview(tgt_len, bsz, embed_dim)\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/functional.py?line=5359'>5360</a>\u001b[0m attn_output \u001b[39m=\u001b[39m linear(attn_output, out_proj_weight, out_proj_bias)\n","File \u001b[1;32mc:\\Users\\bhatt\\anaconda3\\envs\\dLenv\\lib\\site-packages\\torch\\nn\\functional.py:5037\u001b[0m, in \u001b[0;36m_scaled_dot_product_attention\u001b[1;34m(q, k, v, attn_mask, dropout_p)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/functional.py?line=5034'>5035</a>\u001b[0m \u001b[39mif\u001b[39;00m attn_mask \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/functional.py?line=5035'>5036</a>\u001b[0m     attn \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m attn_mask\n\u001b[1;32m-> <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/functional.py?line=5036'>5037</a>\u001b[0m attn \u001b[39m=\u001b[39m softmax(attn, dim\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/functional.py?line=5037'>5038</a>\u001b[0m \u001b[39mif\u001b[39;00m dropout_p \u001b[39m>\u001b[39m \u001b[39m0.0\u001b[39m:\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/functional.py?line=5038'>5039</a>\u001b[0m     attn \u001b[39m=\u001b[39m dropout(attn, p\u001b[39m=\u001b[39mdropout_p)\n","File \u001b[1;32mc:\\Users\\bhatt\\anaconda3\\envs\\dLenv\\lib\\site-packages\\torch\\nn\\functional.py:1818\u001b[0m, in \u001b[0;36msoftmax\u001b[1;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/functional.py?line=1815'>1816</a>\u001b[0m     dim \u001b[39m=\u001b[39m _get_softmax_dim(\u001b[39m\"\u001b[39m\u001b[39msoftmax\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mdim(), _stacklevel)\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/functional.py?line=1816'>1817</a>\u001b[0m \u001b[39mif\u001b[39;00m dtype \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/functional.py?line=1817'>1818</a>\u001b[0m     ret \u001b[39m=\u001b[39m \u001b[39minput\u001b[39;49m\u001b[39m.\u001b[39;49msoftmax(dim)\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/functional.py?line=1818'>1819</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/functional.py?line=1819'>1820</a>\u001b[0m     ret \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msoftmax(dim, dtype\u001b[39m=\u001b[39mdtype)\n","\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 288.00 MiB (GPU 0; 4.00 GiB total capacity; 2.94 GiB already allocated; 0 bytes free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"]}],"source":["evaluate(76)"]},{"cell_type":"code","execution_count":28,"metadata":{"trusted":true},"outputs":[{"ename":"RuntimeError","evalue":"CUDA out of memory. Tried to allocate 288.00 MiB (GPU 0; 4.00 GiB total capacity; 2.97 GiB already allocated; 0 bytes free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[1;32me:\\Final_YearProject\\ImageCaptionGenerator\\transformer-based-image-captioning.ipynb Cell 31'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/Final_YearProject/ImageCaptionGenerator/transformer-based-image-captioning.ipynb#ch0000030?line=0'>1</a>\u001b[0m evaluate(\u001b[39m10\u001b[39;49m)\n","\u001b[1;32me:\\Final_YearProject\\ImageCaptionGenerator\\transformer-based-image-captioning.ipynb Cell 29'\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(index)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Final_YearProject/ImageCaptionGenerator/transformer-based-image-captioning.ipynb#ch0000028?line=12'>13</a>\u001b[0m tar \u001b[39m=\u001b[39m tar\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Final_YearProject/ImageCaptionGenerator/transformer-based-image-captioning.ipynb#ch0000028?line=13'>14</a>\u001b[0m model\u001b[39m.\u001b[39meval()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/Final_YearProject/ImageCaptionGenerator/transformer-based-image-captioning.ipynb#ch0000028?line=14'>15</a>\u001b[0m output \u001b[39m=\u001b[39m  model(src,tar)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Final_YearProject/ImageCaptionGenerator/transformer-based-image-captioning.ipynb#ch0000028?line=15'>16</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(output\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, output\u001b[39m.\u001b[39mshape[\u001b[39m2\u001b[39m]), tar\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m))\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Final_YearProject/ImageCaptionGenerator/transformer-based-image-captioning.ipynb#ch0000028?line=16'>17</a>\u001b[0m sentence_formed \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m\n","File \u001b[1;32mc:\\Users\\bhatt\\anaconda3\\envs\\dLenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","\u001b[1;32me:\\Final_YearProject\\ImageCaptionGenerator\\transformer-based-image-captioning.ipynb Cell 20'\u001b[0m in \u001b[0;36mTransformer.forward\u001b[1;34m(self, src, trg)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Final_YearProject/ImageCaptionGenerator/transformer-based-image-captioning.ipynb#ch0000019?line=69'>70</a>\u001b[0m src_padding_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmake_src_mask(src)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Final_YearProject/ImageCaptionGenerator/transformer-based-image-captioning.ipynb#ch0000019?line=70'>71</a>\u001b[0m trg_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransformer\u001b[39m.\u001b[39mgenerate_square_subsequent_mask(trg_seq_length)\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/Final_YearProject/ImageCaptionGenerator/transformer-based-image-captioning.ipynb#ch0000019?line=71'>72</a>\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer(\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Final_YearProject/ImageCaptionGenerator/transformer-based-image-captioning.ipynb#ch0000019?line=72'>73</a>\u001b[0m     embed_src,\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Final_YearProject/ImageCaptionGenerator/transformer-based-image-captioning.ipynb#ch0000019?line=73'>74</a>\u001b[0m     embed_trg,\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Final_YearProject/ImageCaptionGenerator/transformer-based-image-captioning.ipynb#ch0000019?line=74'>75</a>\u001b[0m     src_key_padding_mask\u001b[39m=\u001b[39;49msrc_padding_mask,\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Final_YearProject/ImageCaptionGenerator/transformer-based-image-captioning.ipynb#ch0000019?line=75'>76</a>\u001b[0m     tgt_mask\u001b[39m=\u001b[39;49mtrg_mask,\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Final_YearProject/ImageCaptionGenerator/transformer-based-image-captioning.ipynb#ch0000019?line=76'>77</a>\u001b[0m )\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Final_YearProject/ImageCaptionGenerator/transformer-based-image-captioning.ipynb#ch0000019?line=77'>78</a>\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc_out(out)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Final_YearProject/ImageCaptionGenerator/transformer-based-image-captioning.ipynb#ch0000019?line=78'>79</a>\u001b[0m \u001b[39mreturn\u001b[39;00m out\n","File \u001b[1;32mc:\\Users\\bhatt\\anaconda3\\envs\\dLenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[1;32mc:\\Users\\bhatt\\anaconda3\\envs\\dLenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:145\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[1;34m(self, src, tgt, src_mask, tgt_mask, memory_mask, src_key_padding_mask, tgt_key_padding_mask, memory_key_padding_mask)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/transformer.py?line=141'>142</a>\u001b[0m \u001b[39mif\u001b[39;00m src\u001b[39m.\u001b[39msize(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39md_model \u001b[39mor\u001b[39;00m tgt\u001b[39m.\u001b[39msize(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39md_model:\n\u001b[0;32m    <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/transformer.py?line=142'>143</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mthe feature number of src and tgt must be equal to d_model\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/transformer.py?line=144'>145</a>\u001b[0m memory \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(src, mask\u001b[39m=\u001b[39;49msrc_mask, src_key_padding_mask\u001b[39m=\u001b[39;49msrc_key_padding_mask)\n\u001b[0;32m    <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/transformer.py?line=145'>146</a>\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder(tgt, memory, tgt_mask\u001b[39m=\u001b[39mtgt_mask, memory_mask\u001b[39m=\u001b[39mmemory_mask,\n\u001b[0;32m    <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/transformer.py?line=146'>147</a>\u001b[0m                       tgt_key_padding_mask\u001b[39m=\u001b[39mtgt_key_padding_mask,\n\u001b[0;32m    <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/transformer.py?line=147'>148</a>\u001b[0m                       memory_key_padding_mask\u001b[39m=\u001b[39mmemory_key_padding_mask)\n\u001b[0;32m    <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/transformer.py?line=148'>149</a>\u001b[0m \u001b[39mreturn\u001b[39;00m output\n","File \u001b[1;32mc:\\Users\\bhatt\\anaconda3\\envs\\dLenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[1;32mc:\\Users\\bhatt\\anaconda3\\envs\\dLenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:202\u001b[0m, in \u001b[0;36mTransformerEncoder.forward\u001b[1;34m(self, src, mask, src_key_padding_mask)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/transformer.py?line=198'>199</a>\u001b[0m output \u001b[39m=\u001b[39m src\n\u001b[0;32m    <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/transformer.py?line=200'>201</a>\u001b[0m \u001b[39mfor\u001b[39;00m mod \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers:\n\u001b[1;32m--> <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/transformer.py?line=201'>202</a>\u001b[0m     output \u001b[39m=\u001b[39m mod(output, src_mask\u001b[39m=\u001b[39;49mmask, src_key_padding_mask\u001b[39m=\u001b[39;49msrc_key_padding_mask)\n\u001b[0;32m    <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/transformer.py?line=203'>204</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/transformer.py?line=204'>205</a>\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm(output)\n","File \u001b[1;32mc:\\Users\\bhatt\\anaconda3\\envs\\dLenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[1;32mc:\\Users\\bhatt\\anaconda3\\envs\\dLenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:344\u001b[0m, in \u001b[0;36mTransformerEncoderLayer.forward\u001b[1;34m(self, src, src_mask, src_key_padding_mask)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/transformer.py?line=341'>342</a>\u001b[0m     x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ff_block(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm2(x))\n\u001b[0;32m    <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/transformer.py?line=342'>343</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/transformer.py?line=343'>344</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm1(x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sa_block(x, src_mask, src_key_padding_mask))\n\u001b[0;32m    <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/transformer.py?line=344'>345</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm2(x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ff_block(x))\n\u001b[0;32m    <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/transformer.py?line=346'>347</a>\u001b[0m \u001b[39mreturn\u001b[39;00m x\n","File \u001b[1;32mc:\\Users\\bhatt\\anaconda3\\envs\\dLenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:352\u001b[0m, in \u001b[0;36mTransformerEncoderLayer._sa_block\u001b[1;34m(self, x, attn_mask, key_padding_mask)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/transformer.py?line=349'>350</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_sa_block\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor,\n\u001b[0;32m    <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/transformer.py?line=350'>351</a>\u001b[0m               attn_mask: Optional[Tensor], key_padding_mask: Optional[Tensor]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/transformer.py?line=351'>352</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself_attn(x, x, x,\n\u001b[0;32m    <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/transformer.py?line=352'>353</a>\u001b[0m                        attn_mask\u001b[39m=\u001b[39;49mattn_mask,\n\u001b[0;32m    <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/transformer.py?line=353'>354</a>\u001b[0m                        key_padding_mask\u001b[39m=\u001b[39;49mkey_padding_mask,\n\u001b[0;32m    <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/transformer.py?line=354'>355</a>\u001b[0m                        need_weights\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)[\u001b[39m0\u001b[39m]\n\u001b[0;32m    <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/transformer.py?line=355'>356</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout1(x)\n","File \u001b[1;32mc:\\Users\\bhatt\\anaconda3\\envs\\dLenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[1;32mc:\\Users\\bhatt\\anaconda3\\envs\\dLenv\\lib\\site-packages\\torch\\nn\\modules\\activation.py:1038\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[1;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/activation.py?line=1026'>1027</a>\u001b[0m     attn_output, attn_output_weights \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mmulti_head_attention_forward(\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/activation.py?line=1027'>1028</a>\u001b[0m         query, key, value, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_dim, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads,\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/activation.py?line=1028'>1029</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39min_proj_weight, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39min_proj_bias,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/activation.py?line=1034'>1035</a>\u001b[0m         q_proj_weight\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mq_proj_weight, k_proj_weight\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mk_proj_weight,\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/activation.py?line=1035'>1036</a>\u001b[0m         v_proj_weight\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mv_proj_weight, average_attn_weights\u001b[39m=\u001b[39maverage_attn_weights)\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/activation.py?line=1036'>1037</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/activation.py?line=1037'>1038</a>\u001b[0m     attn_output, attn_output_weights \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39;49mmulti_head_attention_forward(\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/activation.py?line=1038'>1039</a>\u001b[0m         query, key, value, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membed_dim, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_heads,\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/activation.py?line=1039'>1040</a>\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49min_proj_weight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49min_proj_bias,\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/activation.py?line=1040'>1041</a>\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias_k, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias_v, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madd_zero_attn,\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/activation.py?line=1041'>1042</a>\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mout_proj\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mout_proj\u001b[39m.\u001b[39;49mbias,\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/activation.py?line=1042'>1043</a>\u001b[0m         training\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining,\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/activation.py?line=1043'>1044</a>\u001b[0m         key_padding_mask\u001b[39m=\u001b[39;49mkey_padding_mask, need_weights\u001b[39m=\u001b[39;49mneed_weights,\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/activation.py?line=1044'>1045</a>\u001b[0m         attn_mask\u001b[39m=\u001b[39;49mattn_mask, average_attn_weights\u001b[39m=\u001b[39;49maverage_attn_weights)\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/activation.py?line=1045'>1046</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_first \u001b[39mand\u001b[39;00m is_batched:\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/activation.py?line=1046'>1047</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m attn_output\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m), attn_output_weights\n","File \u001b[1;32mc:\\Users\\bhatt\\anaconda3\\envs\\dLenv\\lib\\site-packages\\torch\\nn\\functional.py:5358\u001b[0m, in \u001b[0;36mmulti_head_attention_forward\u001b[1;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/functional.py?line=5352'>5353</a>\u001b[0m     dropout_p \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/functional.py?line=5354'>5355</a>\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/functional.py?line=5355'>5356</a>\u001b[0m \u001b[39m# (deep breath) calculate attention and out projection\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/functional.py?line=5356'>5357</a>\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[1;32m-> <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/functional.py?line=5357'>5358</a>\u001b[0m attn_output, attn_output_weights \u001b[39m=\u001b[39m _scaled_dot_product_attention(q, k, v, attn_mask, dropout_p)\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/functional.py?line=5358'>5359</a>\u001b[0m attn_output \u001b[39m=\u001b[39m attn_output\u001b[39m.\u001b[39mtranspose(\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mcontiguous()\u001b[39m.\u001b[39mview(tgt_len, bsz, embed_dim)\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/functional.py?line=5359'>5360</a>\u001b[0m attn_output \u001b[39m=\u001b[39m linear(attn_output, out_proj_weight, out_proj_bias)\n","File \u001b[1;32mc:\\Users\\bhatt\\anaconda3\\envs\\dLenv\\lib\\site-packages\\torch\\nn\\functional.py:5034\u001b[0m, in \u001b[0;36m_scaled_dot_product_attention\u001b[1;34m(q, k, v, attn_mask, dropout_p)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/functional.py?line=5031'>5032</a>\u001b[0m q \u001b[39m=\u001b[39m q \u001b[39m/\u001b[39m math\u001b[39m.\u001b[39msqrt(E)\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/functional.py?line=5032'>5033</a>\u001b[0m \u001b[39m# (B, Nt, E) x (B, E, Ns) -> (B, Nt, Ns)\u001b[39;00m\n\u001b[1;32m-> <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/functional.py?line=5033'>5034</a>\u001b[0m attn \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mbmm(q, k\u001b[39m.\u001b[39;49mtranspose(\u001b[39m-\u001b[39;49m\u001b[39m2\u001b[39;49m, \u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m))\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/functional.py?line=5034'>5035</a>\u001b[0m \u001b[39mif\u001b[39;00m attn_mask \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/functional.py?line=5035'>5036</a>\u001b[0m     attn \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m attn_mask\n","\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 288.00 MiB (GPU 0; 4.00 GiB total capacity; 2.97 GiB already allocated; 0 bytes free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"]}],"source":["evaluate(10)"]},{"cell_type":"code","execution_count":29,"metadata":{"trusted":true},"outputs":[{"ename":"RuntimeError","evalue":"CUDA out of memory. Tried to allocate 288.00 MiB (GPU 0; 4.00 GiB total capacity; 3.00 GiB already allocated; 0 bytes free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[1;32me:\\Final_YearProject\\ImageCaptionGenerator\\transformer-based-image-captioning.ipynb Cell 32'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/Final_YearProject/ImageCaptionGenerator/transformer-based-image-captioning.ipynb#ch0000031?line=0'>1</a>\u001b[0m evaluate(\u001b[39m50\u001b[39;49m)\n","\u001b[1;32me:\\Final_YearProject\\ImageCaptionGenerator\\transformer-based-image-captioning.ipynb Cell 29'\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(index)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Final_YearProject/ImageCaptionGenerator/transformer-based-image-captioning.ipynb#ch0000028?line=12'>13</a>\u001b[0m tar \u001b[39m=\u001b[39m tar\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Final_YearProject/ImageCaptionGenerator/transformer-based-image-captioning.ipynb#ch0000028?line=13'>14</a>\u001b[0m model\u001b[39m.\u001b[39meval()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/Final_YearProject/ImageCaptionGenerator/transformer-based-image-captioning.ipynb#ch0000028?line=14'>15</a>\u001b[0m output \u001b[39m=\u001b[39m  model(src,tar)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Final_YearProject/ImageCaptionGenerator/transformer-based-image-captioning.ipynb#ch0000028?line=15'>16</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(output\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, output\u001b[39m.\u001b[39mshape[\u001b[39m2\u001b[39m]), tar\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m))\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Final_YearProject/ImageCaptionGenerator/transformer-based-image-captioning.ipynb#ch0000028?line=16'>17</a>\u001b[0m sentence_formed \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m\n","File \u001b[1;32mc:\\Users\\bhatt\\anaconda3\\envs\\dLenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","\u001b[1;32me:\\Final_YearProject\\ImageCaptionGenerator\\transformer-based-image-captioning.ipynb Cell 20'\u001b[0m in \u001b[0;36mTransformer.forward\u001b[1;34m(self, src, trg)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Final_YearProject/ImageCaptionGenerator/transformer-based-image-captioning.ipynb#ch0000019?line=69'>70</a>\u001b[0m src_padding_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmake_src_mask(src)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Final_YearProject/ImageCaptionGenerator/transformer-based-image-captioning.ipynb#ch0000019?line=70'>71</a>\u001b[0m trg_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransformer\u001b[39m.\u001b[39mgenerate_square_subsequent_mask(trg_seq_length)\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/Final_YearProject/ImageCaptionGenerator/transformer-based-image-captioning.ipynb#ch0000019?line=71'>72</a>\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer(\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Final_YearProject/ImageCaptionGenerator/transformer-based-image-captioning.ipynb#ch0000019?line=72'>73</a>\u001b[0m     embed_src,\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Final_YearProject/ImageCaptionGenerator/transformer-based-image-captioning.ipynb#ch0000019?line=73'>74</a>\u001b[0m     embed_trg,\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Final_YearProject/ImageCaptionGenerator/transformer-based-image-captioning.ipynb#ch0000019?line=74'>75</a>\u001b[0m     src_key_padding_mask\u001b[39m=\u001b[39;49msrc_padding_mask,\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Final_YearProject/ImageCaptionGenerator/transformer-based-image-captioning.ipynb#ch0000019?line=75'>76</a>\u001b[0m     tgt_mask\u001b[39m=\u001b[39;49mtrg_mask,\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Final_YearProject/ImageCaptionGenerator/transformer-based-image-captioning.ipynb#ch0000019?line=76'>77</a>\u001b[0m )\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Final_YearProject/ImageCaptionGenerator/transformer-based-image-captioning.ipynb#ch0000019?line=77'>78</a>\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc_out(out)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Final_YearProject/ImageCaptionGenerator/transformer-based-image-captioning.ipynb#ch0000019?line=78'>79</a>\u001b[0m \u001b[39mreturn\u001b[39;00m out\n","File \u001b[1;32mc:\\Users\\bhatt\\anaconda3\\envs\\dLenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[1;32mc:\\Users\\bhatt\\anaconda3\\envs\\dLenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:145\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[1;34m(self, src, tgt, src_mask, tgt_mask, memory_mask, src_key_padding_mask, tgt_key_padding_mask, memory_key_padding_mask)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/transformer.py?line=141'>142</a>\u001b[0m \u001b[39mif\u001b[39;00m src\u001b[39m.\u001b[39msize(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39md_model \u001b[39mor\u001b[39;00m tgt\u001b[39m.\u001b[39msize(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39md_model:\n\u001b[0;32m    <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/transformer.py?line=142'>143</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mthe feature number of src and tgt must be equal to d_model\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/transformer.py?line=144'>145</a>\u001b[0m memory \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(src, mask\u001b[39m=\u001b[39;49msrc_mask, src_key_padding_mask\u001b[39m=\u001b[39;49msrc_key_padding_mask)\n\u001b[0;32m    <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/transformer.py?line=145'>146</a>\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder(tgt, memory, tgt_mask\u001b[39m=\u001b[39mtgt_mask, memory_mask\u001b[39m=\u001b[39mmemory_mask,\n\u001b[0;32m    <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/transformer.py?line=146'>147</a>\u001b[0m                       tgt_key_padding_mask\u001b[39m=\u001b[39mtgt_key_padding_mask,\n\u001b[0;32m    <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/transformer.py?line=147'>148</a>\u001b[0m                       memory_key_padding_mask\u001b[39m=\u001b[39mmemory_key_padding_mask)\n\u001b[0;32m    <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/transformer.py?line=148'>149</a>\u001b[0m \u001b[39mreturn\u001b[39;00m output\n","File \u001b[1;32mc:\\Users\\bhatt\\anaconda3\\envs\\dLenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[1;32mc:\\Users\\bhatt\\anaconda3\\envs\\dLenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:202\u001b[0m, in \u001b[0;36mTransformerEncoder.forward\u001b[1;34m(self, src, mask, src_key_padding_mask)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/transformer.py?line=198'>199</a>\u001b[0m output \u001b[39m=\u001b[39m src\n\u001b[0;32m    <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/transformer.py?line=200'>201</a>\u001b[0m \u001b[39mfor\u001b[39;00m mod \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers:\n\u001b[1;32m--> <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/transformer.py?line=201'>202</a>\u001b[0m     output \u001b[39m=\u001b[39m mod(output, src_mask\u001b[39m=\u001b[39;49mmask, src_key_padding_mask\u001b[39m=\u001b[39;49msrc_key_padding_mask)\n\u001b[0;32m    <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/transformer.py?line=203'>204</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/transformer.py?line=204'>205</a>\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm(output)\n","File \u001b[1;32mc:\\Users\\bhatt\\anaconda3\\envs\\dLenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[1;32mc:\\Users\\bhatt\\anaconda3\\envs\\dLenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:344\u001b[0m, in \u001b[0;36mTransformerEncoderLayer.forward\u001b[1;34m(self, src, src_mask, src_key_padding_mask)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/transformer.py?line=341'>342</a>\u001b[0m     x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ff_block(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm2(x))\n\u001b[0;32m    <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/transformer.py?line=342'>343</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/transformer.py?line=343'>344</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm1(x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sa_block(x, src_mask, src_key_padding_mask))\n\u001b[0;32m    <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/transformer.py?line=344'>345</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm2(x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ff_block(x))\n\u001b[0;32m    <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/transformer.py?line=346'>347</a>\u001b[0m \u001b[39mreturn\u001b[39;00m x\n","File \u001b[1;32mc:\\Users\\bhatt\\anaconda3\\envs\\dLenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:352\u001b[0m, in \u001b[0;36mTransformerEncoderLayer._sa_block\u001b[1;34m(self, x, attn_mask, key_padding_mask)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/transformer.py?line=349'>350</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_sa_block\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor,\n\u001b[0;32m    <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/transformer.py?line=350'>351</a>\u001b[0m               attn_mask: Optional[Tensor], key_padding_mask: Optional[Tensor]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/transformer.py?line=351'>352</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself_attn(x, x, x,\n\u001b[0;32m    <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/transformer.py?line=352'>353</a>\u001b[0m                        attn_mask\u001b[39m=\u001b[39;49mattn_mask,\n\u001b[0;32m    <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/transformer.py?line=353'>354</a>\u001b[0m                        key_padding_mask\u001b[39m=\u001b[39;49mkey_padding_mask,\n\u001b[0;32m    <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/transformer.py?line=354'>355</a>\u001b[0m                        need_weights\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)[\u001b[39m0\u001b[39m]\n\u001b[0;32m    <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/transformer.py?line=355'>356</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout1(x)\n","File \u001b[1;32mc:\\Users\\bhatt\\anaconda3\\envs\\dLenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[1;32mc:\\Users\\bhatt\\anaconda3\\envs\\dLenv\\lib\\site-packages\\torch\\nn\\modules\\activation.py:1038\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[1;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/activation.py?line=1026'>1027</a>\u001b[0m     attn_output, attn_output_weights \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mmulti_head_attention_forward(\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/activation.py?line=1027'>1028</a>\u001b[0m         query, key, value, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_dim, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads,\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/activation.py?line=1028'>1029</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39min_proj_weight, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39min_proj_bias,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/activation.py?line=1034'>1035</a>\u001b[0m         q_proj_weight\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mq_proj_weight, k_proj_weight\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mk_proj_weight,\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/activation.py?line=1035'>1036</a>\u001b[0m         v_proj_weight\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mv_proj_weight, average_attn_weights\u001b[39m=\u001b[39maverage_attn_weights)\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/activation.py?line=1036'>1037</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/activation.py?line=1037'>1038</a>\u001b[0m     attn_output, attn_output_weights \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39;49mmulti_head_attention_forward(\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/activation.py?line=1038'>1039</a>\u001b[0m         query, key, value, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membed_dim, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_heads,\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/activation.py?line=1039'>1040</a>\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49min_proj_weight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49min_proj_bias,\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/activation.py?line=1040'>1041</a>\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias_k, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias_v, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madd_zero_attn,\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/activation.py?line=1041'>1042</a>\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mout_proj\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mout_proj\u001b[39m.\u001b[39;49mbias,\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/activation.py?line=1042'>1043</a>\u001b[0m         training\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining,\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/activation.py?line=1043'>1044</a>\u001b[0m         key_padding_mask\u001b[39m=\u001b[39;49mkey_padding_mask, need_weights\u001b[39m=\u001b[39;49mneed_weights,\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/activation.py?line=1044'>1045</a>\u001b[0m         attn_mask\u001b[39m=\u001b[39;49mattn_mask, average_attn_weights\u001b[39m=\u001b[39;49maverage_attn_weights)\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/activation.py?line=1045'>1046</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_first \u001b[39mand\u001b[39;00m is_batched:\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/activation.py?line=1046'>1047</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m attn_output\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m), attn_output_weights\n","File \u001b[1;32mc:\\Users\\bhatt\\anaconda3\\envs\\dLenv\\lib\\site-packages\\torch\\nn\\functional.py:5358\u001b[0m, in \u001b[0;36mmulti_head_attention_forward\u001b[1;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/functional.py?line=5352'>5353</a>\u001b[0m     dropout_p \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/functional.py?line=5354'>5355</a>\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/functional.py?line=5355'>5356</a>\u001b[0m \u001b[39m# (deep breath) calculate attention and out projection\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/functional.py?line=5356'>5357</a>\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[1;32m-> <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/functional.py?line=5357'>5358</a>\u001b[0m attn_output, attn_output_weights \u001b[39m=\u001b[39m _scaled_dot_product_attention(q, k, v, attn_mask, dropout_p)\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/functional.py?line=5358'>5359</a>\u001b[0m attn_output \u001b[39m=\u001b[39m attn_output\u001b[39m.\u001b[39mtranspose(\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mcontiguous()\u001b[39m.\u001b[39mview(tgt_len, bsz, embed_dim)\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/functional.py?line=5359'>5360</a>\u001b[0m attn_output \u001b[39m=\u001b[39m linear(attn_output, out_proj_weight, out_proj_bias)\n","File \u001b[1;32mc:\\Users\\bhatt\\anaconda3\\envs\\dLenv\\lib\\site-packages\\torch\\nn\\functional.py:5034\u001b[0m, in \u001b[0;36m_scaled_dot_product_attention\u001b[1;34m(q, k, v, attn_mask, dropout_p)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/functional.py?line=5031'>5032</a>\u001b[0m q \u001b[39m=\u001b[39m q \u001b[39m/\u001b[39m math\u001b[39m.\u001b[39msqrt(E)\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/functional.py?line=5032'>5033</a>\u001b[0m \u001b[39m# (B, Nt, E) x (B, E, Ns) -> (B, Nt, Ns)\u001b[39;00m\n\u001b[1;32m-> <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/functional.py?line=5033'>5034</a>\u001b[0m attn \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mbmm(q, k\u001b[39m.\u001b[39;49mtranspose(\u001b[39m-\u001b[39;49m\u001b[39m2\u001b[39;49m, \u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m))\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/functional.py?line=5034'>5035</a>\u001b[0m \u001b[39mif\u001b[39;00m attn_mask \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/functional.py?line=5035'>5036</a>\u001b[0m     attn \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m attn_mask\n","\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 288.00 MiB (GPU 0; 4.00 GiB total capacity; 3.00 GiB already allocated; 0 bytes free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"]}],"source":["evaluate(50)"]},{"cell_type":"code","execution_count":30,"metadata":{"trusted":true},"outputs":[{"ename":"RuntimeError","evalue":"CUDA out of memory. Tried to allocate 288.00 MiB (GPU 0; 4.00 GiB total capacity; 3.03 GiB already allocated; 0 bytes free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[1;32me:\\Final_YearProject\\ImageCaptionGenerator\\transformer-based-image-captioning.ipynb Cell 33'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/Final_YearProject/ImageCaptionGenerator/transformer-based-image-captioning.ipynb#ch0000032?line=0'>1</a>\u001b[0m evaluate(\u001b[39m40\u001b[39;49m)\n","\u001b[1;32me:\\Final_YearProject\\ImageCaptionGenerator\\transformer-based-image-captioning.ipynb Cell 29'\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(index)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Final_YearProject/ImageCaptionGenerator/transformer-based-image-captioning.ipynb#ch0000028?line=12'>13</a>\u001b[0m tar \u001b[39m=\u001b[39m tar\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Final_YearProject/ImageCaptionGenerator/transformer-based-image-captioning.ipynb#ch0000028?line=13'>14</a>\u001b[0m model\u001b[39m.\u001b[39meval()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/Final_YearProject/ImageCaptionGenerator/transformer-based-image-captioning.ipynb#ch0000028?line=14'>15</a>\u001b[0m output \u001b[39m=\u001b[39m  model(src,tar)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Final_YearProject/ImageCaptionGenerator/transformer-based-image-captioning.ipynb#ch0000028?line=15'>16</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(output\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, output\u001b[39m.\u001b[39mshape[\u001b[39m2\u001b[39m]), tar\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m))\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Final_YearProject/ImageCaptionGenerator/transformer-based-image-captioning.ipynb#ch0000028?line=16'>17</a>\u001b[0m sentence_formed \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m\n","File \u001b[1;32mc:\\Users\\bhatt\\anaconda3\\envs\\dLenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","\u001b[1;32me:\\Final_YearProject\\ImageCaptionGenerator\\transformer-based-image-captioning.ipynb Cell 20'\u001b[0m in \u001b[0;36mTransformer.forward\u001b[1;34m(self, src, trg)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Final_YearProject/ImageCaptionGenerator/transformer-based-image-captioning.ipynb#ch0000019?line=69'>70</a>\u001b[0m src_padding_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmake_src_mask(src)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Final_YearProject/ImageCaptionGenerator/transformer-based-image-captioning.ipynb#ch0000019?line=70'>71</a>\u001b[0m trg_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransformer\u001b[39m.\u001b[39mgenerate_square_subsequent_mask(trg_seq_length)\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/Final_YearProject/ImageCaptionGenerator/transformer-based-image-captioning.ipynb#ch0000019?line=71'>72</a>\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer(\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Final_YearProject/ImageCaptionGenerator/transformer-based-image-captioning.ipynb#ch0000019?line=72'>73</a>\u001b[0m     embed_src,\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Final_YearProject/ImageCaptionGenerator/transformer-based-image-captioning.ipynb#ch0000019?line=73'>74</a>\u001b[0m     embed_trg,\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Final_YearProject/ImageCaptionGenerator/transformer-based-image-captioning.ipynb#ch0000019?line=74'>75</a>\u001b[0m     src_key_padding_mask\u001b[39m=\u001b[39;49msrc_padding_mask,\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Final_YearProject/ImageCaptionGenerator/transformer-based-image-captioning.ipynb#ch0000019?line=75'>76</a>\u001b[0m     tgt_mask\u001b[39m=\u001b[39;49mtrg_mask,\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Final_YearProject/ImageCaptionGenerator/transformer-based-image-captioning.ipynb#ch0000019?line=76'>77</a>\u001b[0m )\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Final_YearProject/ImageCaptionGenerator/transformer-based-image-captioning.ipynb#ch0000019?line=77'>78</a>\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc_out(out)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Final_YearProject/ImageCaptionGenerator/transformer-based-image-captioning.ipynb#ch0000019?line=78'>79</a>\u001b[0m \u001b[39mreturn\u001b[39;00m out\n","File \u001b[1;32mc:\\Users\\bhatt\\anaconda3\\envs\\dLenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[1;32mc:\\Users\\bhatt\\anaconda3\\envs\\dLenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:145\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[1;34m(self, src, tgt, src_mask, tgt_mask, memory_mask, src_key_padding_mask, tgt_key_padding_mask, memory_key_padding_mask)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/transformer.py?line=141'>142</a>\u001b[0m \u001b[39mif\u001b[39;00m src\u001b[39m.\u001b[39msize(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39md_model \u001b[39mor\u001b[39;00m tgt\u001b[39m.\u001b[39msize(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39md_model:\n\u001b[0;32m    <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/transformer.py?line=142'>143</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mthe feature number of src and tgt must be equal to d_model\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/transformer.py?line=144'>145</a>\u001b[0m memory \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(src, mask\u001b[39m=\u001b[39;49msrc_mask, src_key_padding_mask\u001b[39m=\u001b[39;49msrc_key_padding_mask)\n\u001b[0;32m    <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/transformer.py?line=145'>146</a>\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder(tgt, memory, tgt_mask\u001b[39m=\u001b[39mtgt_mask, memory_mask\u001b[39m=\u001b[39mmemory_mask,\n\u001b[0;32m    <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/transformer.py?line=146'>147</a>\u001b[0m                       tgt_key_padding_mask\u001b[39m=\u001b[39mtgt_key_padding_mask,\n\u001b[0;32m    <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/transformer.py?line=147'>148</a>\u001b[0m                       memory_key_padding_mask\u001b[39m=\u001b[39mmemory_key_padding_mask)\n\u001b[0;32m    <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/transformer.py?line=148'>149</a>\u001b[0m \u001b[39mreturn\u001b[39;00m output\n","File \u001b[1;32mc:\\Users\\bhatt\\anaconda3\\envs\\dLenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[1;32mc:\\Users\\bhatt\\anaconda3\\envs\\dLenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:202\u001b[0m, in \u001b[0;36mTransformerEncoder.forward\u001b[1;34m(self, src, mask, src_key_padding_mask)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/transformer.py?line=198'>199</a>\u001b[0m output \u001b[39m=\u001b[39m src\n\u001b[0;32m    <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/transformer.py?line=200'>201</a>\u001b[0m \u001b[39mfor\u001b[39;00m mod \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers:\n\u001b[1;32m--> <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/transformer.py?line=201'>202</a>\u001b[0m     output \u001b[39m=\u001b[39m mod(output, src_mask\u001b[39m=\u001b[39;49mmask, src_key_padding_mask\u001b[39m=\u001b[39;49msrc_key_padding_mask)\n\u001b[0;32m    <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/transformer.py?line=203'>204</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/transformer.py?line=204'>205</a>\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm(output)\n","File \u001b[1;32mc:\\Users\\bhatt\\anaconda3\\envs\\dLenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[1;32mc:\\Users\\bhatt\\anaconda3\\envs\\dLenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:344\u001b[0m, in \u001b[0;36mTransformerEncoderLayer.forward\u001b[1;34m(self, src, src_mask, src_key_padding_mask)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/transformer.py?line=341'>342</a>\u001b[0m     x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ff_block(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm2(x))\n\u001b[0;32m    <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/transformer.py?line=342'>343</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/transformer.py?line=343'>344</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm1(x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sa_block(x, src_mask, src_key_padding_mask))\n\u001b[0;32m    <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/transformer.py?line=344'>345</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm2(x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ff_block(x))\n\u001b[0;32m    <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/transformer.py?line=346'>347</a>\u001b[0m \u001b[39mreturn\u001b[39;00m x\n","File \u001b[1;32mc:\\Users\\bhatt\\anaconda3\\envs\\dLenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:352\u001b[0m, in \u001b[0;36mTransformerEncoderLayer._sa_block\u001b[1;34m(self, x, attn_mask, key_padding_mask)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/transformer.py?line=349'>350</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_sa_block\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor,\n\u001b[0;32m    <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/transformer.py?line=350'>351</a>\u001b[0m               attn_mask: Optional[Tensor], key_padding_mask: Optional[Tensor]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/transformer.py?line=351'>352</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself_attn(x, x, x,\n\u001b[0;32m    <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/transformer.py?line=352'>353</a>\u001b[0m                        attn_mask\u001b[39m=\u001b[39;49mattn_mask,\n\u001b[0;32m    <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/transformer.py?line=353'>354</a>\u001b[0m                        key_padding_mask\u001b[39m=\u001b[39;49mkey_padding_mask,\n\u001b[0;32m    <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/transformer.py?line=354'>355</a>\u001b[0m                        need_weights\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)[\u001b[39m0\u001b[39m]\n\u001b[0;32m    <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/transformer.py?line=355'>356</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout1(x)\n","File \u001b[1;32mc:\\Users\\bhatt\\anaconda3\\envs\\dLenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[1;32mc:\\Users\\bhatt\\anaconda3\\envs\\dLenv\\lib\\site-packages\\torch\\nn\\modules\\activation.py:1038\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[1;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/activation.py?line=1026'>1027</a>\u001b[0m     attn_output, attn_output_weights \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mmulti_head_attention_forward(\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/activation.py?line=1027'>1028</a>\u001b[0m         query, key, value, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_dim, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads,\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/activation.py?line=1028'>1029</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39min_proj_weight, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39min_proj_bias,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/activation.py?line=1034'>1035</a>\u001b[0m         q_proj_weight\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mq_proj_weight, k_proj_weight\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mk_proj_weight,\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/activation.py?line=1035'>1036</a>\u001b[0m         v_proj_weight\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mv_proj_weight, average_attn_weights\u001b[39m=\u001b[39maverage_attn_weights)\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/activation.py?line=1036'>1037</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/activation.py?line=1037'>1038</a>\u001b[0m     attn_output, attn_output_weights \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39;49mmulti_head_attention_forward(\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/activation.py?line=1038'>1039</a>\u001b[0m         query, key, value, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membed_dim, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_heads,\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/activation.py?line=1039'>1040</a>\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49min_proj_weight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49min_proj_bias,\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/activation.py?line=1040'>1041</a>\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias_k, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias_v, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madd_zero_attn,\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/activation.py?line=1041'>1042</a>\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mout_proj\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mout_proj\u001b[39m.\u001b[39;49mbias,\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/activation.py?line=1042'>1043</a>\u001b[0m         training\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining,\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/activation.py?line=1043'>1044</a>\u001b[0m         key_padding_mask\u001b[39m=\u001b[39;49mkey_padding_mask, need_weights\u001b[39m=\u001b[39;49mneed_weights,\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/activation.py?line=1044'>1045</a>\u001b[0m         attn_mask\u001b[39m=\u001b[39;49mattn_mask, average_attn_weights\u001b[39m=\u001b[39;49maverage_attn_weights)\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/activation.py?line=1045'>1046</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_first \u001b[39mand\u001b[39;00m is_batched:\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/modules/activation.py?line=1046'>1047</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m attn_output\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m), attn_output_weights\n","File \u001b[1;32mc:\\Users\\bhatt\\anaconda3\\envs\\dLenv\\lib\\site-packages\\torch\\nn\\functional.py:5358\u001b[0m, in \u001b[0;36mmulti_head_attention_forward\u001b[1;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/functional.py?line=5352'>5353</a>\u001b[0m     dropout_p \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/functional.py?line=5354'>5355</a>\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/functional.py?line=5355'>5356</a>\u001b[0m \u001b[39m# (deep breath) calculate attention and out projection\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/functional.py?line=5356'>5357</a>\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[1;32m-> <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/functional.py?line=5357'>5358</a>\u001b[0m attn_output, attn_output_weights \u001b[39m=\u001b[39m _scaled_dot_product_attention(q, k, v, attn_mask, dropout_p)\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/functional.py?line=5358'>5359</a>\u001b[0m attn_output \u001b[39m=\u001b[39m attn_output\u001b[39m.\u001b[39mtranspose(\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mcontiguous()\u001b[39m.\u001b[39mview(tgt_len, bsz, embed_dim)\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/functional.py?line=5359'>5360</a>\u001b[0m attn_output \u001b[39m=\u001b[39m linear(attn_output, out_proj_weight, out_proj_bias)\n","File \u001b[1;32mc:\\Users\\bhatt\\anaconda3\\envs\\dLenv\\lib\\site-packages\\torch\\nn\\functional.py:5034\u001b[0m, in \u001b[0;36m_scaled_dot_product_attention\u001b[1;34m(q, k, v, attn_mask, dropout_p)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/functional.py?line=5031'>5032</a>\u001b[0m q \u001b[39m=\u001b[39m q \u001b[39m/\u001b[39m math\u001b[39m.\u001b[39msqrt(E)\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/functional.py?line=5032'>5033</a>\u001b[0m \u001b[39m# (B, Nt, E) x (B, E, Ns) -> (B, Nt, Ns)\u001b[39;00m\n\u001b[1;32m-> <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/functional.py?line=5033'>5034</a>\u001b[0m attn \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mbmm(q, k\u001b[39m.\u001b[39;49mtranspose(\u001b[39m-\u001b[39;49m\u001b[39m2\u001b[39;49m, \u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m))\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/functional.py?line=5034'>5035</a>\u001b[0m \u001b[39mif\u001b[39;00m attn_mask \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   <a href='file:///c%3A/Users/bhatt/anaconda3/envs/dLenv/lib/site-packages/torch/nn/functional.py?line=5035'>5036</a>\u001b[0m     attn \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m attn_mask\n","\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 288.00 MiB (GPU 0; 4.00 GiB total capacity; 3.03 GiB already allocated; 0 bytes free; 3.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"]}],"source":["evaluate(40)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"interpreter":{"hash":"430cf71330057c3667661f8009a18fe65a7652eec0a123d908058e168adcc71d"},"kernelspec":{"display_name":"Python 3.9.0 ('dLenv')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.0"}},"nbformat":4,"nbformat_minor":4}
